<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Machinelearning : For the Coursera Machine Learning course">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Machinelearning</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/dkd58/MachineLearning">View on GitHub</a>

          <h1 id="project_title">Machinelearning</h1>
          <h2 id="project_tagline">For the Coursera Machine Learning course</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/dkd58/MachineLearning/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/dkd58/MachineLearning/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p>This is an explanation of how I arrived at the solution for the Course Project of the Machine Learning Coursera course (Johns Hopkins University).</p>

<h2>
<a id="preparation-steps" class="anchor" href="#preparation-steps" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preparation steps</h2>

<ul>
<li>Read data: training and test dataset (the latter containing the 20 observations without the classe variable, that need to be classified and submitted in the second part of the course project)</li>
<li>After checking the contents of the training dataset (dimensions, variables, table of classe variable) Is plit this set into a real training set (75%) and a separate test set, in order to be able to cross validate the error estimates.</li>
<li>Next, I checked the occurrence of missing values (NAs): there are a lot of these. I decided to remove all the variables that where completely missing - i.e. on all observations - of the 20-row "official" test set, because these variables could never contribute to accurate prediction of the classe variable on this set.</li>
<li>The remaining 60 (out of 160) variables are completely observed; the first 7 "administrative" variables are not relevant for prediction, however, so these are removed as well. This leaves 53 variables, of which the last one is the classe variable.</li>
<li>Checking for NearZeroVariance variable did not reveal any problems. Outlier detection resulted in removal of 1 observation.</li>
</ul>

<h2>
<a id="modeling" class="anchor" href="#modeling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Modeling</h2>

<ul>
<li>In order to reduce the set of features (variables) further, I tried to get a feeling for which features were most tightly correlated with the classe variable. However, the featurePlots were not showing very clear relationships, neither did the boxplots of each of the features against the classe variable.</li>
<li>I therefore treid reducing dimensionality using PCA. This resulted in 13 PC's explaining 80% of the variance.</li>
<li>Unfortunately, plotting the PC's against each other while colouring the observations by classe, also did not reveal immediately the set of PC's to use.</li>
<li>I decided to have a go at modeling the classe variable as a function of the 13 PC's using the linear discriminant method "lda" in the train function of the caret package. This resulted in an estimated .470 accuracy (more or less confirmed on the test set with .466). Strangely enough, using the train fuunction with the preProcess option "pca" gave a better performance (.531) than a separate PCA followed by a call to train without the preprocessing (?)</li>
<li>When applying the "lda" method to the full 52 variables (instead of the PC's) I arrived at an accuracy of .70, so much higher than using PC's.</li>
<li>Trying a multinomial regression model via the train function was very slow, and it did not improve upon the lda method: accuracy = .665.
It was interesting to try a classification tree on this problem (rpart), but it did not do better than the lda method: only .485</li>
<li>Finally, a random forest model was estimated using the randomForest function (the train function with method "rf" is quite a bit slower on a problem this size). This proved to deliver a very high accuracy, of &gt;.99, which was confirmed on the independent test set. This seemed incredible at first; I thought I must have made a mistake. I even randomised the row order in the train dataset in order to see whether perhaps the row ordering had been used by the RF; but this proved not to be the case...</li>
<li>Conclusion: for this problem a random forest is able to deliver a near-perfect classification, using only 60 out of 160 variables.</li>
</ul>

<h1>
<a id="one-remark-because-of-the-incredibly-high-accuracy-99-we-get-using-the" class="anchor" href="#one-remark-because-of-the-incredibly-high-accuracy-99-we-get-using-the" aria-hidden="true"><span class="octicon octicon-link"></span></a>One remark because of the incredibly high accuracy (&gt;.99) we get using the</h1>

<h1>
<a id="random-forest-on-the-test-set-could-it-be-that-the-row-number-is-used-because-the" class="anchor" href="#random-forest-on-the-test-set-could-it-be-that-the-row-number-is-used-because-the" aria-hidden="true"><span class="octicon octicon-link"></span></a>random forest on the test set. Could it be that the row number is used (because the</h1>

<h1>
<a id="rows-are-ordered-by-class-a-e-apparently-not" class="anchor" href="#rows-are-ordered-by-class-a-e-apparently-not" aria-hidden="true"><span class="octicon octicon-link"></span></a>rows are ordered by class A-E?). Apparently not:</h1>

<h1>
<a id="if-we-randomly-sort-the-rows-so-that-classes-a-e-are-not-grouped" class="anchor" href="#if-we-randomly-sort-the-rows-so-that-classes-a-e-are-not-grouped" aria-hidden="true"><span class="octicon octicon-link"></span></a>If we randomly sort the rows, so that classes A-E are not grouped</h1>

<h1>
<a id="we-still-get-a-great-result-using-random-forests" class="anchor" href="#we-still-get-a-great-result-using-random-forests" aria-hidden="true"><span class="octicon octicon-link"></span></a>we still get a great result using random forests</h1>

<h1>
<a id="in-this-case-it-cannot-handle-52-columns-because-they-are-double" class="anchor" href="#in-this-case-it-cannot-handle-52-columns-because-they-are-double" aria-hidden="true"><span class="octicon octicon-link"></span></a>in this case it cannot handle 52 columns (because they are "double"?)</h1>

<h1>
<a id="but-even-using-11-columns-we-achieve-an-amazing-92-oob-error-rate" class="anchor" href="#but-even-using-11-columns-we-achieve-an-amazing-92-oob-error-rate" aria-hidden="true"><span class="octicon octicon-link"></span></a>but even using 11 columns we achieve an amazing 92% OOB error rate</h1>

<h1>
<a id="this-is-confirmed-by-the-result-on-the-testset" class="anchor" href="#this-is-confirmed-by-the-result-on-the-testset" aria-hidden="true"><span class="octicon octicon-link"></span></a>this is confirmed by the result on the testset</h1>

<h1>
<a id="using-the-original-dataset-so-not-resorting-it-we-get-a-99-generalization-error" class="anchor" href="#using-the-original-dataset-so-not-resorting-it-we-get-a-99-generalization-error" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using the original dataset (so not resorting it) we get a &gt;99% generalization error)</h1>

<h1>
<a id="because-we-can-use-all-52-columns" class="anchor" href="#because-we-can-use-all-52-columns" aria-hidden="true"><span class="octicon octicon-link"></span></a>because we can use all 52 columns</h1>

<h1>
<a id="the-other-techniques-do-not-go-further-than-approx-70" class="anchor" href="#the-other-techniques-do-not-go-further-than-approx-70" aria-hidden="true"><span class="octicon octicon-link"></span></a>the other techniques do not go further than approx. 70%</h1>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Machinelearning maintained by <a href="https://github.com/dkd58">dkd58</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
