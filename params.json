{"name":"Machinelearning","tagline":"For the Coursera Machine Learning course","body":"This is an explanation of how I arrived at the solution for the Course Project of the Machine Learning Coursera course (Johns Hopkins University).\r\n\r\n##Preparation steps\r\n* Read data: training and test dataset (the latter containing the 20 observations without the classe variable, that need to be classified and submitted in the second part of the course project)\r\n* After checking the contents of the training dataset (dimensions, variables, table of classe variable) Is plit this set into a real training set (75%) and a separate test set, in order to be able to cross validate the error estimates.\r\n* Next, I checked the occurrence of missing values (NAs): there are a lot of these. I decided to remove all the variables that where completely missing - i.e. on all observations - of the 20-row \"official\" test set, because these variables could never contribute to accurate prediction of the classe variable on this set.\r\n* The remaining 60 (out of 160) variables are completely observed; the first 7 \"administrative\" variables are not relevant for prediction, however, so these are removed as well. This leaves 53 variables, of which the last one is the classe variable.\r\n* Checking for NearZeroVariance variable did not reveal any problems. Outlier detection resulted in removal of 1 observation.\r\n\r\n##Modeling\r\n* In order to reduce the set of features (variables) further, I tried to get a feeling for which features were most tightly correlated with the classe variable. However, the featurePlots were not showing very clear relationships, neither did the boxplots of each of the features against the classe variable.\r\n* I therefore treid reducing dimensionality using PCA. This resulted in 13 PC's explaining 80% of the variance.\r\n* Unfortunately, plotting the PC's against each other while colouring the observations by classe, also did not reveal immediately the set of PC's to use.\r\n* I decided to have a go at modeling the classe variable as a function of the 13 PC's using the linear discriminant method \"lda\" in the train function of the caret package. This resulted in an estimated .470 accuracy (more or less confirmed on the test set with .466). Strangely enough, using the train fuunction with the preProcess option \"pca\" gave a better performance (.531) than a separate PCA followed by a call to train without the preprocessing (?)\r\n* When applying the \"lda\" method to the full 52 variables (instead of the PC's) I arrived at an accuracy of .70, so much higher than using PC's.\r\n* Trying a multinomial regression model via the train function was very slow, and it did not improve upon the lda method: accuracy = .665.\r\nIt was interesting to try a classification tree on this problem (rpart), but it did not do better than the lda method: only .485\r\n* Finally, a random forest model was estimated using the randomForest function (the train function with method \"rf\" is quite a bit slower on a problem this size). This proved to deliver a very high accuracy, of >.99, which was confirmed on the independent test set. This seemed incredible at first; I thought I must have made a mistake. I even randomised the row order in the train dataset in order to see whether perhaps the row ordering had been used by the RF; but this proved not to be the case...\r\n* Conclusion: for this problem a random forest is able to deliver a near-perfect classification, using only 60 out of 160 variables.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# One remark because of the incredibly high accuracy (>.99) we get using the \r\n# random forest on the test set. Could it be that the row number is used (because the\r\n# rows are ordered by class A-E?). Apparently not:\r\n# If we randomly sort the rows, so that classes A-E are not grouped\r\n# we still get a great result using random forests \r\n# in this case it cannot handle 52 columns (because they are \"double\"?)\r\n# but even using 11 columns we achieve an amazing 92% OOB error rate\r\n# this is confirmed by the result on the testset\r\n# Using the original dataset (so not resorting it) we get a >99% generalization error)\r\n# because we can use all 52 columns\r\n# the other techniques do not go further than approx. 70%\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}